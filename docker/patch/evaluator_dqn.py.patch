--- evaluator.py	2022-09-12 00:18:43.040378668 +0900
+++ evaluator_dqn_patch.py	2023-01-11 16:05:08.323098591 +0900
@@ -8,6 +8,8 @@
 
 import pfrl
 
+import matplotlib.pyplot as plt
+
 
 def _run_episodes(
     env,
@@ -516,6 +518,26 @@
     def evaluate_if_necessary(self, t, episodes):
         if t >= self.prev_eval_t + self.eval_interval:
             score = self.evaluate_and_update_max_score(t, episodes)
+            scores = np.loadtxt(self.outdir+"/scores.txt", skiprows=1, unpack=True)
+            data01_axis1 = scores[0]
+            data01_value1 = scores[3]
+            data01_aveq = scores[8]
+            data01_loss = scores[9]
+            fig_1 = plt.figure(figsize=(12, 6))
+            ax_1 = fig_1.add_subplot(111)
+            ax_1.plot(data01_axis1, data01_value1,  color="r", label="score")
+            ax_1.set_xlabel("step")
+            ax_1.set_ylabel("reward")
+            ax_1.legend(loc="upper left")
+            plt.savefig(self.outdir+"/reward_graph.png", dpi=300)
+            fig_2 = plt.figure(figsize=(12, 6))
+            ax_2 = fig_2.add_subplot(111)
+            ax_2.plot(data01_axis1, data01_aveq,  color="c", label="average_q")
+            ax_2.plot(data01_axis1, data01_loss,  color="m", label="average_loss")
+            ax_2.set_xlabel("step")
+            ax_2.set_ylabel("loss")
+            ax_2.legend(loc="upper left")
+            plt.savefig(self.outdir+"/log_graph.png", dpi=300)
             self.prev_eval_t = t - t % self.eval_interval
             return score
         return None
@@ -672,3 +694,4 @@
 
     def join_tensorboard_writer(self):
         self.record_tb_stats_thread.join()
+
