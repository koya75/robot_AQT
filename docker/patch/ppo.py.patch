--- ppo.py	2022-09-27 10:29:34.784958914 +0900
+++ ppo_patch.py	2022-09-27 10:28:35.239446762 +0900
@@ -673,7 +673,9 @@
 
         return loss
 
-    def batch_act(self, batch_obs):
+    def batch_act(self, batch_obs, item, demo=False):
+        self.item=item
+        self.demo = demo
         if self.training:
             return self._batch_act_train(batch_obs)
         else:
@@ -695,14 +697,28 @@
         with torch.no_grad(), pfrl.utils.evaluating(self.model):
             if self.recurrent:
                 (action_distrib, _), self.test_recurrent_states = one_step_forward(
-                    self.model, b_state, self.test_recurrent_states
+                    self.model, b_state, self.test_recurrent_states, self.item
                 )
+                if self.act_deterministically:
+                    action = mode_of_distribution(action_distrib).cpu().numpy()
+                else:
+                    action = action_distrib.sample().cpu().numpy()
+            elif self.demo:
+                from vit_pytorch.recorder import Recorder
+                model = Recorder(self.model)
+                preds, attns = model(b_state)
+                action_distrib, _ = preds
+                if self.act_deterministically:
+                    action = mode_of_distribution(action_distrib).cpu().numpy()
+                else:
+                    action = action_distrib.sample().cpu().numpy()
+                action = (action, attns)
             else:
                 action_distrib, _ = self.model(b_state)
-            if self.act_deterministically:
-                action = mode_of_distribution(action_distrib).cpu().numpy()
-            else:
-                action = action_distrib.sample().cpu().numpy()
+                if self.act_deterministically:
+                    action = mode_of_distribution(action_distrib).cpu().numpy()
+                else:
+                    action = action_distrib.sample().cpu().numpy()
 
         return action
 
@@ -729,7 +745,7 @@
                     (action_distrib, batch_value),
                     self.train_recurrent_states,
                 ) = one_step_forward(
-                    self.model, b_state, self.train_prev_recurrent_states
+                    self.model, b_state, self.train_prev_recurrent_states, self.item
                 )
             else:
                 action_distrib, batch_value = self.model(b_state)
